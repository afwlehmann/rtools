% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pca.R
\name{pca.reduced}
\alias{pca.reduced}
\title{Principal Component Analysis of the given high-dimensional dataset X.}
\usage{
pca.reduced(X, center = TRUE, retX = TRUE)
}
\arguments{
\item{X}{a matrix whose rows contain the multivariate samples}

\item{center}{a boolean indicating whether the data should be centered}

\item{retX}{whether to compute and return a projection of the (possibly
centered) original dataset onto its principal components}
}
\value{
a list consisting of
  \item{\code{X}}{if \code{retX} is \code{TRUE}, a projection of the (possibly
    centered original dataset onto its principal components, else \code{NULL}}
  \item{\code{V}}{an orthonormal matrix whose columns resemble the
    principal components, i.e. the eigenvectors of the covariance matrix}
  \item{\code{sd}}{the standard deviations of the principal components, i.e.
    the square roots of the eigenvalues of the covariance matrix}
  \item{\code{mean}}{mean of the original dataset}
}
\description{
This function is intended for datasets whose dimensionality \eqn{M} exceeds
the number of samples \eqn{N} by several magnitudes, i.e. \eqn{M \gg N}{M >> N}.
}
\details{
The basic idea is to compute the eigenvectors of the covariance matrix of
the \emph{transposed} dataset \eqn{X^T}. Note that in this scenario, where
\eqn{M \gg N}{M >> N}, there are at most \eqn{N} linearly independent
eigenvectors.

By working on the transposed dataset, the size of the corresponding
covariance matrix is reduced from \eqn{M \times M}{M x M} to \eqn{N \times
N}{N x N}, which, under the given circumstances, is a huge computational
advantage.

Let \eqn{\Sigma := X^T X}{Sigma := X^T X} be the covariance matrix of \eqn{X}
(note that both the mean \eqn{\mu}{mu} as well as the factor
\eqn{\frac{1}{N-1}}{1/(N-1)} have been dropped for increased readability),
then
\deqn{
  \begin{array}{lrcl}
                    &    X^T X x & = & \lambda x   \\
    \Leftrightarrow &  X X^T X x & = & \lambda X x
  \end{array}
}{
      X^T X x = lambda x
<=> X X^T X x = lambda X x
}

Substitution of \eqn{y} for \eqn{X x} yields
\deqn{X X^T y = \lambda y}{X X^T y = lambda y}

Then
\deqn{
  \begin{array}{lrcl}
                    &   X^T X X^T y & = & \lambda X^T y \\
    \Leftrightarrow & (X^T X) X^T y & = & \lambda X^T y \\
    \Leftrightarrow &  \Sigma X^T y & = & \lambda X^T y
  \end{array}
}{
      X^T X X^T y = lambda X^T y
<=> (X^T X) X^T y = lambda X^T y
<=>   Sigma X^T y = lambda X^T y
}

from which it follows that \eqn{x = X^T y} is an eigenvector of the original
covariance matrix \eqn{\Sigma}{Sigma}.

Hence, first the eigenvectors \eqn{y} of \eqn{X X^T} are calculated and then
transformed by \eqn{X^T} in order to compute the eigenvectors \eqn{x} of
\eqn{X^T X = \Sigma}{X^T X = Sigma}.
}

